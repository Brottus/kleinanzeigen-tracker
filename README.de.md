<div align="right">

**ğŸ‡©ğŸ‡ª Deutsch** | **[ğŸ‡¬ğŸ‡§ English](README.md)**

</div>

# Ebay Kleinanzeigen Monitoring System

<div align="center">
  <img src="assets/banner.svg" alt="kleinanzeigen tracker" width="800">
</div>

<br>

[![Create Release](https://github.com/Brottus/kleinanzeigen-tracker/actions/workflows/release.yml/badge.svg)](https://github.com/Brottus/kleinanzeigen-tracker/actions/workflows/release.yml)
[![Build and Push Docker Images](https://github.com/Brottus/kleinanzeigen-tracker/actions/workflows/build-and-push.yml/badge.svg)](https://github.com/Brottus/kleinanzeigen-tracker/actions/workflows/build-and-push.yml)
[![CodeQL](https://github.com/Brottus/kleinanzeigen-tracker/actions/workflows/github-code-scanning/codeql/badge.svg?branch=main)](https://github.com/Brottus/kleinanzeigen-tracker/actions/workflows/github-code-scanning/codeql)
[![Dependabot Status](https://img.shields.io/badge/Dependabot-aktiviert-success?logo=dependabot)](https://github.com/Brottus/ebaykleinanzeigen/network/updates)

[![Lizenz: AGPL v3](https://img.shields.io/badge/Lizenz-AGPL%20v3-blue.svg)](https://www.gnu.org/licenses/agpl-3.0)
[![Python](https://img.shields.io/badge/Python-3.14+-3776AB?logo=python&logoColor=white)](https://www.python.org/)
[![Docker](https://img.shields.io/badge/Docker-Bereit-2496ED?logo=docker&logoColor=white)](https://www.docker.com/)

[![Docker Pulls - Scraper](https://img.shields.io/docker/pulls/brottus/ebay-kleinanzeigen-scraper?logo=docker&label=Scraper%20Downloads)](https://hub.docker.com/r/brottus/ebay-kleinanzeigen-scraper)
[![Docker Pulls - Scheduler](https://img.shields.io/docker/pulls/brottus/ebay-kleinanzeigen-job-scheduler?logo=docker&label=Scheduler%20Downloads)](https://hub.docker.com/r/brottus/ebay-kleinanzeigen-job-scheduler)
[![Docker Image Size - Scraper](https://img.shields.io/docker/image-size/brottus/ebay-kleinanzeigen-scraper/latest?logo=docker&label=Scraper%20GrÃ¶ÃŸe)](https://hub.docker.com/r/brottus/ebay-kleinanzeigen-scraper)
[![Docker Image Size - Scheduler](https://img.shields.io/docker/image-size/brottus/ebay-kleinanzeigen-job-scheduler/latest?logo=docker&label=Scheduler%20GrÃ¶ÃŸe)](https://hub.docker.com/r/brottus/ebay-kleinanzeigen-job-scheduler)

Ein umfassendes Microservices-basiertes System zur Ãœberwachung und zum Scraping von Kleinanzeigen auf kleinanzeigen.de (ehemals eBay Kleinanzeigen) mit automatischer Job-Planung und optionalen Matterbridge-Benachrichtigungen.

---

<div align="center">

### ğŸ¤– UnterstÃ¼tze dieses Projekt

*Dieses Projekt wurde mit KI-UnterstÃ¼tzung entwickelt. Wenn du es nÃ¼tzlich findest, spendiere mir ein paar KI-Token!* â˜•

[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/brottus)

</div>

---

## ğŸ¯ ProjektÃ¼bersicht

Dieses System besteht aus zwei Hauptdiensten, die zusammenarbeiten, um eine automatisierte Ãœberwachung von kleinanzeigen.de-Anzeigen zu ermÃ¶glichen:

1. **Ebay Kleinanzeigen Scraper** - Produktionsreife API zum Extrahieren von Anzeigendaten
2. **Ebay Kleinanzeigen Job Scheduler** - Automatische Job-Planung mit Web-Dashboard

### Hauptfunktionen

- âœ… **Umfassende Datenextraktion** - 15 Felder pro Anzeige inkl. Bilder, Preise, Standorte und mehr
- âœ… **Multi-URL-UnterstÃ¼tzung** - Scrape mehrere Such-URLs gleichzeitig mit automatischer Deduplizierung
- âœ… **Automatische Planung** - Cron-basierte Job-Planung mit APScheduler
- âœ… **Anti-Detection** - User-Agent-Rotation, zufÃ¤llige VerzÃ¶gerungen und automatische Wiederholungen
- âœ… **Echtzeit-Benachrichtigungen** - Matterbridge-Integration fÃ¼r sofortige Benachrichtigungen (Discord, Slack, Teams, etc.)
- âœ… **Web-Dashboard** - Moderne SPA mit JWT-Authentifizierung
- âœ… **REST API** - VollstÃ¤ndige OpenAPI/Swagger-Dokumentation
- âœ… **Docker-UnterstÃ¼tzung** - Produktionsreife Containerisierung
- âœ… **Produktionsbereit** - Gunicorn WSGI-Server, Health-Checks, Logging

## ğŸ“Š Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Docker-Netzwerk                           â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚     Scraper          â”‚      â”‚       Scheduler      â”‚    â”‚
â”‚  â”‚  Port: 3000          â”‚â—„â”€â”€â”€â”€â”€â”¤ Port: 3001           â”‚    â”‚
â”‚  â”‚                      â”‚      â”‚                      â”‚    â”‚
â”‚  â”‚  â€¢ Datenextraktion   â”‚      â”‚  â€¢ Job-Verwaltung    â”‚    â”‚
â”‚  â”‚  â€¢ API-Key-Auth      â”‚      â”‚  â€¢ Web-Dashboard     â”‚    â”‚
â”‚  â”‚  â€¢ Multi-URL-Support â”‚      â”‚  â€¢ JWT-Auth          â”‚    â”‚
â”‚  â”‚  â€¢ Anti-Detection    â”‚      â”‚  â€¢ Benachrichtigungenâ”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                              â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                              â”‚
            â–¼                              â–¼
     kleinanzeigen.de                Matterbridge
                                (fÃ¼r Benachrichtigungen)
```

## ğŸš€ Schnellstart

### Voraussetzungen

- Docker (fÃ¼r Produktionsbereitstellung)
- Docker Compose (nur fÃ¼r Entwicklung/Tests)
- ODER Python 3.11+ (fÃ¼r manuelle Installation)

### Produktionsbereitstellung (Empfohlen)

**Jeden Dienst in einem eigenen Docker-Container ausfÃ¼hren:**

1. **Scraper API erstellen und ausfÃ¼hren**
```bash
cd ebay-kleinanzeigen-scraper
docker build -t kleinanzeigen-scraper-api .
docker run -d \
  -p 3000:3000 \
  -e API_KEYS=dein-sicherer-api-key \
  --name scraper-api \
  kleinanzeigen-scraper-api
```

2. **Job Scheduler erstellen und ausfÃ¼hren**
```bash
cd ebay-kleinanzeigen-job-scheduler
docker build -t kleinanzeigen-job-scheduler .
docker run -d \
  -p 3001:3001 \
  -v scheduler-data:/app/data \
  -e ADMIN_PASSWORD=dein-sicheres-passwort \
  -e SESSION_SECRET=$(openssl rand -base64 32) \
  -e JWT_SECRET=$(openssl rand -base64 32) \
  -e SCRAPER_API_URL=http://scraper-api:3000 \
  -e SCRAPER_API_KEY=dein-sicherer-api-key \
  --link scraper-api \
  --name job-scheduler \
  kleinanzeigen-job-scheduler
```

3. **Auf die Dienste zugreifen**
- Scraper API: http://localhost:3000
- Scraper Dokumentation: http://localhost:3000/docs
- Scheduler Dashboard: http://localhost:3001
- Scheduler API Dokumentation: http://localhost:3001/docs

4. **Standard-Zugangsdaten**
- Benutzername: `admin`
- Passwort: Ãœber `ADMIN_PASSWORD` festgelegt

### Entwicklung/Tests mit Docker Compose

**Nur fÃ¼r Entwicklung und Tests:**

```bash
# Repository klonen
git clone <repository-url>
cd ebaykleinanzeigen

# 1. Matterbridge konfigurieren (falls Benachrichtigungen verwendet werden)
mkdir -p matterbridge
# Erstelle matterbridge.toml im matterbridge/ Ordner
# Siehe: https://github.com/42wim/matterbridge/wiki/How-to-create-your-config

# 2. Umgebungsvariablen setzen (optional)
export MATTERBRIDGE_TOKEN="dein-matterbridge-api-token"
export MATTERBRIDGE_GATEWAY="gateway_ebaykleinanzeigen"

# 3. docker-compose.yml bearbeiten, um Secrets zu setzen (SESSION_SECRET, JWT_SECRET, etc.)

# 4. Alle Dienste starten (einschlieÃŸlich Matterbridge)
docker-compose up -d
```

**Enthaltene Dienste:**
- `scraper` - Scraper API (Port 3000)
- `scheduler` - Job Scheduler (Port 3001)
- `matterbridge` - Message Bridge fÃ¼r Benachrichtigungen (Port 4242)

**âš ï¸ Hinweis:** Docker Compose wird nur fÃ¼r Entwicklung/Tests empfohlen. FÃ¼r die Produktion verwende individuelle Docker-Container wie oben gezeigt.

### Manuelle Installation

Siehe individuelle Service README-Dateien:
- [Scraper Setup](./ebay-kleinanzeigen-scraper/README.de.md)
- [Scheduler Setup](./ebay-kleinanzeigen-job-scheduler/README.de.md)

## ğŸ“š Dienst-Dokumentation

### Ebay Kleinanzeigen Scraper API

**Zweck:** Anzeigendaten von kleinanzeigen.de-Suchseiten extrahieren

**Wichtige Endpunkte:**
- `GET /api/scrape` - Anzeigen von URL(s) scrapen
- `GET /api/newest` - Nur neueste Anzeige abrufen
- `GET /health` - Health-Check

**Funktionen:**
- 15-Felder-Datenextraktion (ID, Titel, Preis, Standort, Bilder, etc.)
- Multi-URL-Scraping mit Deduplizierung
- `since`-Parameter fÃ¼r inkrementelle Updates
- API-Key-Authentifizierung
- User-Agent-Rotation & Anti-Detection
- Docker-Bereitstellung verfÃ¼gbar

[VollstÃ¤ndige Dokumentation â†’](./ebay-kleinanzeigen-scraper/README.de.md)

### Ebay Kleinanzeigen Job Scheduler

**Zweck:** Ãœberwachung mit geplanten Jobs und Benachrichtigungen automatisieren

**Hauptfunktionen:**
- Cron-basierte Job-Planung
- Web-Dashboard mit JWT-Authentifizierung
- Matterbridge-Integration fÃ¼r Benachrichtigungen
- Job-Historie und Statusverfolgung
- Manuelle Job-AusfÃ¼hrung
- Docker-Bereitstellung verfÃ¼gbar

**Endpunkte:**
- `POST /api/auth/login` - Authentifizieren
- `GET /api/jobs` - Jobs auflisten
- `POST /api/jobs` - Job erstellen
- `POST /api/jobs/{id}/run` - Job manuell ausfÃ¼hren

[VollstÃ¤ndige Dokumentation â†’](./ebay-kleinanzeigen-job-scheduler/README.de.md)

## ğŸ”§ Konfiguration

### Umgebungsvariablen

#### Scraper API
```bash
PORT=3000                          # Server-Port
API_KEYS=key1,key2                # Kommagetrennte API-Keys
LOG_LEVEL=INFO                     # Logging-Level
ENABLE_SWAGGER_UI=true            # API-Dokumentation aktivieren
```

#### Job Scheduler
```bash
PORT=3001                          # Server-Port
ADMIN_USERNAME=admin              # Admin-Benutzername
ADMIN_PASSWORD=admin              # Admin-Passwort (Ã¤ndern!)
SESSION_SECRET=random-secret      # Session-VerschlÃ¼sselungskey
JWT_SECRET=random-jwt-secret      # JWT-Signaturkey
ENABLE_SWAGGER_UI=true            # API-Dokumentation aktivieren
ENABLE_WEB_UI=true                # Web-Dashboard aktivieren

# Scraper API Verbindung
SCRAPER_API_URL=http://scraper:3000
SCRAPER_API_KEY=test-key-123

# Matterbridge (Erforderlich fÃ¼r Benachrichtigungen)
MATTERBRIDGE_URL=http://ip:4242
MATTERBRIDGE_TOKEN=dein-token
MATTERBRIDGE_GATEWAY=gateway_name
NOTIFICATION_LANGUAGE=de          # de oder en
```

### Matterbridge-Einrichtung

**Matterbridge** ist eine Message-Bridge, die Benachrichtigungen an verschiedene Chat-Plattformen weiterleitet (Discord, Slack, Teams, Telegram, IRC, Matrix, etc.).

**Dokumentation:**
- Hauptrepository: https://github.com/42wim/matterbridge
- Wiki & Setup-Anleitung: https://github.com/42wim/matterbridge/wiki
- Konfigurationsbeispiele: https://github.com/42wim/matterbridge/wiki/How-to-create-your-config

#### Option 1: Docker Compose verwenden (Entwicklung/Tests)

Die enthaltene `docker-compose.yml` hat Matterbridge bereits konfiguriert:

```bash
# 1. Matterbridge-Konfigurationsverzeichnis erstellen
mkdir -p matterbridge

# 2. matterbridge.toml-Konfigurationsdatei erstellen
# Siehe offizielle Anleitung: https://github.com/42wim/matterbridge/wiki/How-to-create-your-config
nano matterbridge/matterbridge.toml

# 3. API-Token setzen (falls API-Gateway verwendet wird)
export MATTERBRIDGE_TOKEN="dein-api-token"

# 4. Alle Dienste starten
docker-compose up -d

# 5. Matterbridge ist nun verfÃ¼gbar unter http://localhost:4242
```

#### Option 2: EigenstÃ¤ndiges Matterbridge (Produktion)

```bash
# Matterbridge separat ausfÃ¼hren
docker run -d \
  -p 4242:4242 \
  -v /pfad/zur/config:/etc/matterbridge:ro \
  --name matterbridge \
  42wim/matterbridge:stable

# Job Scheduler konfigurieren, um sich damit zu verbinden
docker run -d \
  -e MATTERBRIDGE_URL=http://matterbridge:4242 \
  -e MATTERBRIDGE_TOKEN=dein-token \
  -e MATTERBRIDGE_GATEWAY=dein-gateway \
  --link matterbridge \
  kleinanzeigen-job-scheduler
```

#### Konfigurationsschritte

1. **Matterbridge installieren** (via Docker wie oben gezeigt)
2. **Konfigurationsdatei erstellen** `matterbridge.toml` - [Konfigurationsanleitung](https://github.com/42wim/matterbridge/wiki/How-to-create-your-config)
3. **Gateway einrichten** (Discord, Slack, etc.) in der Config
4. **API-Token abrufen** von deiner Matterbridge-Instanz
5. **Job Scheduler konfigurieren:**
   - `MATTERBRIDGE_URL`, `MATTERBRIDGE_TOKEN`, `MATTERBRIDGE_GATEWAY` setzen
   - Oder Dashboard-Konfigurationsreiter verwenden
6. **Benachrichtigungen aktivieren** fÃ¼r einzelne Jobs

FÃ¼r detaillierte Matterbridge-Konfiguration siehe das [offizielle Wiki](https://github.com/42wim/matterbridge/wiki).

## ğŸ’¡ Verwendungsbeispiele

### Beispiel 1: MÃ¼nchner MÃ¶bel Ã¼berwachen

1. **Beim Scheduler Dashboard anmelden**
```
http://localhost:3001
Benutzername: admin
Passwort: admin
```

2. **Einen Job erstellen**
- Name: "MÃ¼nchner Tische"
- URL: `/s-wohnzimmer/muenchen/tisch/k0c88l6411`
- Zeitplan: `*/30 * * * *` (alle 30 Minuten)
- Benachrichtigungen aktivieren: Ja

3. **Job lÃ¤uft automatisch** und benachrichtigt dich Ã¼ber neue Anzeigen!

### Beispiel 2: API-Nutzung

**Anzeigen scrapen:**
```bash
curl -H "X-API-Key: test-key-123" \
  "http://localhost:3000/api/scrape?url=/s-autos/c216"
```

**Nur neue Anzeigen seit letzter PrÃ¼fung abrufen:**
```bash
curl -H "X-API-Key: test-key-123" \
  "http://localhost:3000/api/scrape?url=/s-autos/c216&since=3287237963"
```

**Job via API erstellen:**
```bash
# 1. Anmelden
TOKEN=$(curl -X POST http://localhost:3001/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username":"admin","password":"admin"}' \
  | jq -r '.access_token')

# 2. Job erstellen
curl -X POST http://localhost:3001/api/jobs \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Autos Ã¼berwachen",
    "url": "/s-autos/c216",
    "schedule": "*/30 * * * *",
    "notify_enabled": true
  }'
```

## ğŸ“– API-Dokumentation

Beide Dienste bieten interaktive Swagger UI-Dokumentation:

- **Scraper API Dokumentation:** http://localhost:3000/docs
- **Scheduler API Dokumentation:** http://localhost:3001/docs

## ğŸ”’ Sicherheit

### Produktions-Checkliste

- [ ] Standard-Admin-Passwort Ã¤ndern
- [ ] Starke `SESSION_SECRET` und `JWT_SECRET` generieren
- [ ] Sichere API-Keys verwenden (nicht `test-key-123`)
- [ ] `FLASK_DEBUG=false` setzen
- [ ] Firewall-Regeln konfigurieren
- [ ] HTTPS-Reverse-Proxy verwenden (nginx/traefik)
- [ ] AbhÃ¤ngigkeiten regelmÃ¤ÃŸig aktualisieren
- [ ] ErwÃ¤ge Swagger UI in Produktion zu deaktivieren (`ENABLE_SWAGGER_UI=false`)

### Authentifizierung

**Scraper API:**
- API-Key-Authentifizierung via `X-API-Key`-Header
- Keys konfiguriert via `API_KEYS`-Umgebungsvariable

**Job Scheduler:**
- JWT-basierte Authentifizierung
- Access Tokens (1 Stunde) + Refresh Tokens (7 Tage)
- Sliding Window Refresh

## ğŸ› Fehlerbehebung

### Scraper API startet nicht
```bash
# Logs prÃ¼fen
docker logs ebay-kleinanzeigen-scraper

# HÃ¤ufige Probleme:
# - Port 3000 bereits in Verwendung
# - Fehlende API_KEYS-Umgebungsvariable
```

### Job Scheduler kann sich nicht mit Scraper API verbinden
```bash
# PrÃ¼fen, ob Scraper API lÃ¤uft
curl http://localhost:3000/health

# Netzwerkverbindung prÃ¼fen
docker exec ebay-kleinanzeigen-job-scheduler curl http://scraper:3000/health

# PrÃ¼fen, ob API-Key in beiden Diensten Ã¼bereinstimmt
```

### Jobs werden nicht ausgefÃ¼hrt
```bash
# Scheduler-Logs prÃ¼fen
docker logs ebay-kleinanzeigen-job-scheduler

# Cron-Ausdruck validieren
# Verwende https://crontab.guru zur Validierung

# PrÃ¼fen, ob Job im Dashboard aktiviert ist
```

## ğŸ“Š Monitoring

### Health Checks

```bash
# Scraper API
curl http://localhost:3000/health

# Job Scheduler
curl http://localhost:3001/health

# Alle Dienste via Scheduler API
curl -H "Authorization: Bearer $TOKEN" \
  http://localhost:3001/api/health/services
```

### Logs

```bash
# Echtzeit-Logs anzeigen
docker-compose logs -f

# Nur Scraper API
docker logs -f ebay-kleinanzeigen-scraper

# Nur Job Scheduler
docker logs -f ebay-kleinanzeigen-job-scheduler
```

## ğŸ”„ Updates

```bash
# Neueste Ã„nderungen abrufen
git pull

# Neu erstellen und neu starten
docker-compose down
docker-compose build --no-cache
docker-compose up -d
```

## ğŸ“ Lizenz

Dieses Projekt ist unter der **GNU Affero General Public License v3.0 (AGPL-3.0)** lizenziert.

**Was das bedeutet:**
- âœ… **Kostenlos nutzbar** - Private und kommerzielle Nutzung erlaubt
- âœ… **Frei modifizierbar** - Code nach Bedarf Ã¤ndern und anpassen
- âœ… **Verbesserungen teilen** - Alle Ã„nderungen mÃ¼ssen unter AGPL-3.0 zurÃ¼ck geteilt werden
- âœ… **Netzwerknutzung offenlegen** - Bei Betrieb als Webdienst muss Quellcode Nutzern zur VerfÃ¼gung gestellt werden

Diese Lizenz stellt sicher, dass das Projekt Open Source bleibt und Verbesserungen allen zugutekommen.

Siehe [LICENSE](LICENSE)-Datei fÃ¼r vollstÃ¤ndige Details oder besuche https://www.gnu.org/licenses/agpl-3.0.html

### Drittanbieter-Hinweise

Dieses Projekt verwendet viele hervorragende Open-Source-Bibliotheken und Tools. Siehe [NOTICE.md](NOTICE.md) fÃ¼r eine vollstÃ¤ndige Liste der AbhÃ¤ngigkeiten, Urheberrechtsinhaber und deren Lizenzen.

## ğŸ¤ Mitwirken

BeitrÃ¤ge sind willkommen! Bitte:
1. Repository forken
2. Feature-Branch erstellen
3. Ã„nderungen vornehmen
4. Pull Request einreichen

## âš ï¸ Haftungsausschluss

Dieses Tool dient nur zu Bildungszwecken. Respektiere immer die Nutzungsbedingungen und robots.txt von kleinanzeigen.de. Verwende es verantwortungsvoll und Ã¼berlade ihre Server nicht.

## ğŸ“§ Support

FÃ¼r Probleme und Fragen:
- Ã–ffne ein GitHub Issue
- PrÃ¼fe bestehende Dokumentation
- Sieh dir die API-Dokumentation unter `/docs`-Endpunkten an

## ğŸŒŸ Star History

Wenn du dieses Projekt nÃ¼tzlich findest, gib ihm bitte einen Stern! â­
